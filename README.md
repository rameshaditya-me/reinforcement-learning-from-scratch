# Reinforcement Learning From Scratch

This lecture extends the wonderful series by [norhum](https://github.com/norhum/reinforcement-learning-from-scratch) with additional topics and math equations. This lecture series covers the following key topics and algorithms:

*   **Introduction to RL:** Agent-Environment interaction loop, States, Actions, Rewards, Policy.
  
*   **Multi-Armed Bandits (MAB):**
    *   The exploration-exploitation dilemma.
    *   Epsilon-Greedy strategy.
    *   Epislion-Greedy strategy with scheduling.
    *   Upper Confidence Bound (UCB) strategy.
    *   Stochastic Bandits.
    *   Neural Bandits
      
*   **Value-Based Methods:**
    *   Q-Values vs. Rewards.
    *   Q-Learning (Off-policy TD learning).
    *   SARSA (On-policy TD learning).
      
*   **Deep Reinforcement Learning:**
    *   Using Neural Networks for Function Approximation (Deep Q-Networks - DQN).
    *   Addressing instability with Experience Replay and Target Networks.
    *   Applying DQN to a more challenging GridWorld environment.
      
*   **Policy Gradient Methods:**
    *   Introduction to the Gymnasium library (CartPole environment).
    *   REINFORCE algorithm (Monte Carlo policy gradients).
      
*   **Actor-Critic Methods:**
    *   Introduction to the Pendulum environment.
    *   Advantage Actor-Critic (A2C) algorithm.
